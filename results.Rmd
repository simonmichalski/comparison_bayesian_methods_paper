
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
```

```{r}
library("dplyr")

df_results <- readRDS(file.path("results", "results.rds"))
sim_based_thres <- readRDS(file.path("results", "sim_thres.rds"))
df_recovery <- readRDS(file.path("results", "recovery.rds"))
df_sim_thres_prior <- readRDS(file.path("results", "sim_thres_per_prior_sd.rds"))

# Correlation true-estimated subject-level parameters
data_cor_s_log_k <- summarize(group_by(df_recovery, s_log_k_sd, prior_sd, sample), 
                              correlation = cor(true_s_log_k, median_s_log_k))
median_cor_s_log_k_overall <- median(data_cor_s_log_k$correlation)
iqr_cor_s_log_k_overall <- IQR(data_cor_s_log_k$correlation)
median_cor_s_log_k_0_2 <- median(data_cor_s_log_k$correlation[data_cor_s_log_k$s_log_k_sd == 0.2])
iqr_cor_s_log_k_0_2 <- IQR(data_cor_s_log_k$correlation[data_cor_s_log_k$s_log_k_sd == 0.2])
median_cor_s_log_k_0_51 <- median(data_cor_s_log_k$correlation[data_cor_s_log_k$s_log_k_sd == 0.51])
iqr_cor_s_log_k_0_51 <- IQR(data_cor_s_log_k$correlation[data_cor_s_log_k$s_log_k_sd == 0.51])
median_cor_s_log_k_0_81 <- median(data_cor_s_log_k$correlation[data_cor_s_log_k$s_log_k_sd == 0.81])
iqr_cor_s_log_k_0_81 <- IQR(data_cor_s_log_k$correlation[data_cor_s_log_k$s_log_k_sd == 0.81])

data_cor_log_k <- summarize(group_by(df_recovery, s_log_k_sd, prior_sd, sample), 
                            correlation = cor(true_log_k, median_log_k))
median_cor_log_k_overall <- median(data_cor_log_k$correlation)
iqr_cor_log_k_overall <- IQR(data_cor_log_k$correlation)

# Subject-level parameter shrinkage
sd_estimates_s_log_k <- aggregate(df_recovery, median_s_log_k ~ prior_sd + s_log_k_sd + sample, sd)
sd_true_s_log_k <- aggregate(df_recovery, true_s_log_k ~ prior_sd + s_log_k_sd + sample, sd)
data_shrinkage_s_log_k <- merge(sd_estimates_s_log_k, sd_true_s_log_k, by = c("s_log_k_sd", "prior_sd", "sample"))
data_shrinkage_s_log_k$sd_reduction <- (data_shrinkage_s_log_k$true_s_log_k - data_shrinkage_s_log_k$median_s_log_k) / data_shrinkage_s_log_k$true_s_log_k
mean_shrinkage_overall <- mean(data_shrinkage_s_log_k$sd_reduction)
sd_shrinkage_overall <- sd(data_shrinkage_s_log_k$sd_reduction)
mean_shrinkage_0_2 <- mean(data_shrinkage_s_log_k[data_shrinkage_s_log_k$s_log_k_sd == 0.2, "sd_reduction"])
sd_shrinkage_0_2 <- sd(data_shrinkage_s_log_k[data_shrinkage_s_log_k$s_log_k_sd == 0.2, "sd_reduction"])
mean_shrinkage_0_51 <-mean(data_shrinkage_s_log_k[data_shrinkage_s_log_k$s_log_k_sd == 0.51, "sd_reduction"])
sd_shrinkage_0_51 <-sd(data_shrinkage_s_log_k[data_shrinkage_s_log_k$s_log_k_sd == 0.51, "sd_reduction"])
mean_shrinkage_0_81 <-mean(data_shrinkage_s_log_k[data_shrinkage_s_log_k$s_log_k_sd == 0.81, "sd_reduction"])
sd_shrinkage_0_81 <-sd(data_shrinkage_s_log_k[data_shrinkage_s_log_k$s_log_k_sd == 0.81, "sd_reduction"])
mean_sd_true_0_2 <- mean(sd_true_s_log_k[sd_true_s_log_k$s_log_k_sd == 0.2, "true_s_log_k"])
sd_sd_true_0_2 <- sd(sd_true_s_log_k[sd_true_s_log_k$s_log_k_sd == 0.2, "true_s_log_k"])
mean_sd_true_0_51 <- mean(sd_true_s_log_k[sd_true_s_log_k$s_log_k_sd == 0.51, "true_s_log_k"])
sd_sd_true_0_51 <- sd(sd_true_s_log_k[sd_true_s_log_k$s_log_k_sd == 0.51, "true_s_log_k"])
mean_sd_true_0_81 <- mean(sd_true_s_log_k[sd_true_s_log_k$s_log_k_sd == 0.81, "true_s_log_k"])
sd_sd_true_0_81 <- sd(sd_true_s_log_k[sd_true_s_log_k$s_log_k_sd == 0.81, "true_s_log_k"])
mean_sd_estimates_0_2 <- mean(sd_estimates_s_log_k[sd_estimates_s_log_k$s_log_k_sd == 0.2, "median_s_log_k"])
sd_sd_estimates_0_2 <- sd(sd_estimates_s_log_k[sd_estimates_s_log_k$s_log_k_sd == 0.2, "median_s_log_k"])
mean_sd_estimates_0_51 <- mean(sd_estimates_s_log_k[sd_estimates_s_log_k$s_log_k_sd == 0.51, "median_s_log_k"])
sd_sd_estimates_0_51 <- sd(sd_estimates_s_log_k[sd_estimates_s_log_k$s_log_k_sd == 0.51, "median_s_log_k"])
mean_sd_estimates_0_81 <- mean(sd_estimates_s_log_k[sd_estimates_s_log_k$s_log_k_sd == 0.81, "median_s_log_k"])
sd_sd_estimates_0_81 <- sd(sd_estimates_s_log_k[sd_estimates_s_log_k$s_log_k_sd == 0.81, "median_s_log_k"])

sd_estimates_log_k <- aggregate(df_recovery, median_log_k ~ prior_sd + s_log_k_sd + sample, sd)
sd_true_log_k <- aggregate(df_recovery, true_log_k ~ prior_sd + s_log_k_sd + sample, sd)
data_shrinkage_log_k <- merge(sd_estimates_log_k, sd_true_log_k, by = c("s_log_k_sd", "prior_sd", "sample"))
data_shrinkage_log_k$sd_reduction <- (data_shrinkage_log_k$true_log_k - data_shrinkage_log_k$median_log_k) / data_shrinkage_log_k$true_log_k
mean_shrinkage_overall_log_k <- mean(data_shrinkage_log_k$sd_reduction)
sd_shrinkage_overall_log_k <- sd(data_shrinkage_log_k$sd_reduction)
mean_sd_true_overall_log_k <- mean(sd_true_log_k$true_log_k)
sd_sd_true_overall_log_k <- sd(sd_true_log_k$true_log_k)
mean_sd_estimates_overall_log_k <- mean(sd_estimates_log_k$median_log_k)
sd_sd_estimates_overall_log_k <- sd(sd_estimates_log_k$median_log_k)

# Recovery group-level mean
true_means_s_log_k <- aggregate(df_recovery, true_s_log_k ~ prior_sd + s_log_k_sd + sample, mean)
mean_true_group_level_mean_overall <- mean(true_means_s_log_k$true_s_log_k)
sd_true_group_level_mean_overall <- sd(true_means_s_log_k$true_s_log_k)
mean_true_group_level_mean_0_2 <- mean(true_means_s_log_k[true_means_s_log_k$s_log_k_sd == 0.2, "true_s_log_k"])
sd_true_group_level_mean_0_2 <- sd(true_means_s_log_k[true_means_s_log_k$s_log_k_sd == 0.2, "true_s_log_k"])
mean_true_group_level_mean_0_51 <- mean(true_means_s_log_k[true_means_s_log_k$s_log_k_sd == 0.51, "true_s_log_k"])
sd_true_group_level_mean_0_51 <- sd(true_means_s_log_k[true_means_s_log_k$s_log_k_sd == 0.51, "true_s_log_k"])
mean_true_group_level_mean_0_81 <- mean(true_means_s_log_k[true_means_s_log_k$s_log_k_sd == 0.81, "true_s_log_k"])
sd_true_group_level_mean_0_81 <- sd(true_means_s_log_k[true_means_s_log_k$s_log_k_sd == 0.81, "true_s_log_k"])

true_means_log_k <- aggregate(df_recovery, true_log_k ~ prior_sd + s_log_k_sd + sample, mean)
mean_true_group_level_mean_log_k <- mean(true_means_log_k$true_log_k)
sd_true_group_level_mean_log_k <- sd(true_means_log_k$true_log_k)

true_abs_mean_sample_means_s_log_k <- aggregate(true_means_s_log_k, abs(true_s_log_k) ~ s_log_k_sd, mean)

mean_group_level_mean_overall <- mean(df_results$median_mu_s_log_k)
sd_group_level_mean_overall <- sd(df_results$median_mu_s_log_k)
mean_group_level_mean_0_2 <- mean(df_results[df_results$s_log_k_sd == 0.2, "median_mu_s_log_k"])
sd_group_level_mean_0_2 <- sd(df_results[df_results$s_log_k_sd == 0.2, "median_mu_s_log_k"])
mean_group_level_mean_0_51 <- mean(df_results[df_results$s_log_k_sd == 0.51, "median_mu_s_log_k"])
sd_group_level_mean_0_51 <- sd(df_results[df_results$s_log_k_sd == 0.51, "median_mu_s_log_k"])
mean_group_level_mean_0_81 <- mean(df_results[df_results$s_log_k_sd == 0.81, "median_mu_s_log_k"])
sd_group_level_mean_0_81 <- sd(df_results[df_results$s_log_k_sd == 0.81, "median_mu_s_log_k"])

mean_group_level_mean_log_k <- mean(df_results$median_mu_log_k)
sd_group_level_mean_log_k <- sd(df_results$median_mu_log_k)

# Recovery group-level SD
mean_group_level_sd_0_2 <- mean(df_results[df_results$s_log_k_sd == 0.2, "median_sd_s_log_k"])
sd_group_level_sd_0_2 <- sd(df_results[df_results$s_log_k_sd == 0.2, "median_sd_s_log_k"])
mean_group_level_sd_0_51 <- mean(df_results[df_results$s_log_k_sd == 0.51, "median_sd_s_log_k"])
sd_group_level_sd_0_51 <- sd(df_results[df_results$s_log_k_sd == 0.51, "median_sd_s_log_k"])
mean_group_level_sd_0_81 <- mean(df_results[df_results$s_log_k_sd == 0.81, "median_sd_s_log_k"])
sd_group_level_sd_0_81 <- sd(df_results[df_results$s_log_k_sd == 0.81, "median_sd_s_log_k"])

mean_group_level_sd_log_k <- mean(df_results$median_sd_log_k)
sd_group_level_sd_log_k <- sd(df_results$median_sd_log_k)


# False positive results
fp_savage_dickey_bf_3 <- sum(df_results$fp_savage_dickey_bf_3)/nrow(df_results)

fp_directional_bf_3 <- sum(df_results$fp_directional_bf_3_pos,
                           df_results$fp_directional_bf_1_3_neg)/nrow(df_results)

fp_directional_bf_10 <- sum(df_results$fp_directional_bf_10_pos,
                            df_results$fp_directional_bf_1_10_neg)/nrow(df_results)

fp_directional_bf_30 <- sum(df_results$fp_directional_bf_30_pos,
                            df_results$fp_directional_bf_1_30_neg)/nrow(df_results)

fp_directional_bf_100 <- sum(df_results$fp_directional_bf_100_pos,
                             df_results$fp_directional_bf_1_100_neg)/nrow(df_results)

fp_p_effect_95 <- sum(df_results$fp_p_effect_95_pos,
                      df_results$fp_p_effect_05_neg)/nrow(df_results)

fp_p_effect_975 <- sum(df_results$fp_p_effect_975_pos,
                       df_results$fp_p_effect_025_neg)/nrow(df_results)

fp_p_effect_99 <- sum(df_results$fp_p_effect_99_pos,
                      df_results$fp_p_effect_01_neg)/nrow(df_results)

fp_hdi_80 <- sum(df_results$fp_hdi_80_pos,
                 df_results$fp_hdi_80_neg)/nrow(df_results)

fp_hdi_90 <- sum(df_results$fp_hdi_90_pos,
                 df_results$fp_hdi_90_neg)/nrow(df_results)

fp_hdi_95 <- sum(df_results$fp_hdi_95_pos,
                 df_results$fp_hdi_95_neg)/nrow(df_results)

fp_hdi_99 <- sum(df_results$fp_hdi_99_pos,
                 df_results$fp_hdi_99_neg)/nrow(df_results)

# False positives per prior SD
get_fp_per_prior_sd <- function(column_pos, column_neg){
  prior_sds <- c(0.05, 0.1, 0.2, 0.5, 1, 1.5, 2, 2.5)
  
  vector_fps <- sapply(prior_sds, function(sd){
    sum(df_results[df_results$prior_sd == sd,column_pos],df_results[df_results$prior_sd == sd,column_neg])/600
  })
  
  # Convert to rounded percentages
  sapply(vector_fps, function(proportion){
    format(round(proportion*100, 2), nsmall = 2)
  })
}

fp_savage_dickey_bf_3_priors <- get_fp_per_prior_sd("fp_savage_dickey_bf_3", "")
fp_directional_bf_3_priors <- get_fp_per_prior_sd("fp_directional_bf_3_pos", "fp_directional_bf_1_3_neg")
fp_directional_bf_10_priors <- get_fp_per_prior_sd("fp_directional_bf_10_pos", "fp_directional_bf_1_10_neg")
fp_directional_bf_30_priors <- get_fp_per_prior_sd("fp_directional_bf_30_pos", "fp_directional_bf_1_30_neg")
fp_directional_bf_100_priors <- get_fp_per_prior_sd("fp_directional_bf_100_pos", "fp_directional_bf_1_100_neg")
fp_p_effect_95_priors <- get_fp_per_prior_sd("fp_p_effect_95_pos", "fp_p_effect_05_neg")
fp_p_effect_975_priors <- get_fp_per_prior_sd("fp_p_effect_975_pos", "fp_p_effect_025_neg")
fp_p_effect_99_priors <- get_fp_per_prior_sd("fp_p_effect_99_pos", "fp_p_effect_01_neg")
fp_hdi_80_priors <- get_fp_per_prior_sd("fp_hdi_80_pos", "fp_hdi_80_neg")
fp_hdi_90_priors <- get_fp_per_prior_sd("fp_hdi_90_pos", "fp_hdi_90_neg")
fp_hdi_95_priors <- get_fp_per_prior_sd("fp_hdi_95_pos", "fp_hdi_95_neg")
fp_hdi_99_priors <- get_fp_per_prior_sd("fp_hdi_99_pos", "fp_hdi_99_neg")

# Overlap of false positive results (ofp)
# Number of false positives per method
n_fp_savage_dickey_bf <- sum(df_results$fp_savage_dickey_bf_sim)
n_fp_directional_bf <- sum(df_results$fp_directional_bf_sim_pos, df_results$fp_directional_bf_sim_neg)
n_fp_p_effect <- sum(df_results$fp_p_effect_sim_pos, df_results$fp_p_effect_sim_neg)
n_fp_hdi <- sum(df_results$fp_hdi_sim_pos, df_results$fp_hdi_sim_neg)

ofp_savage_dickey_bf_directional_bf <- sum(df_results$fp_savage_dickey_bf_sim == 1 &
                                             (df_results$fp_directional_bf_sim_pos == 1 |
                                             df_results$fp_directional_bf_sim_neg == 1)) / n_fp_savage_dickey_bf

ofp_savage_dickey_bf_hdi <- sum(df_results$fp_savage_dickey_bf_sim == 1 &
                                  (df_results$fp_hdi_sim_pos == 1 |
                                  df_results$fp_hdi_sim_neg == 1)) / n_fp_savage_dickey_bf

ofp_savage_dickey_bf_p_effect <- sum(df_results$fp_savage_dickey_bf_sim == 1 &
                                       (df_results$fp_p_effect_sim_pos == 1 |
                                       df_results$fp_p_effect_sim_neg == 1)) / n_fp_savage_dickey_bf

ofp_directional_bf_hdi <- sum(df_results$fp_directional_bf_sim_pos == 1 &
                                df_results$fp_hdi_sim_pos == 1,
                              df_results$fp_directional_bf_sim_neg == 1 &
                                df_results$fp_hdi_sim_neg == 1) / n_fp_directional_bf

ofp_directional_bf_p_effect <- sum(df_results$fp_directional_bf_sim_pos == 1 &
                                     df_results$fp_p_effect_sim_pos == 1,
                                   df_results$fp_directional_bf_sim_neg == 1 &
                                     df_results$fp_p_effect_sim_neg == 1) / n_fp_directional_bf

ofp_hdi_p_effect <- sum(df_results$fp_hdi_sim_pos == 1 &
                          df_results$fp_p_effect_sim_pos == 1,
                        df_results$fp_hdi_sim_neg == 1 &
                          df_results$fp_p_effect_sim_neg == 1) / n_fp_hdi


# Median within sample range (mwsr)
get_within_sample_range <- function(column){
  range_vec <- vector()
  for (i in seq(from = 1, by = 8, length.out = length(column)/8)){
    range_vec <- append(range_vec, range(column[seq(i, i+7)])[2] - range(column[seq(i, i+7)])[1])
  }
  return(range_vec)
}

mwsr_savage_dickey_bf <- median(get_within_sample_range(df_results$savage_dickey_bf))
mwsr_directional_bf <- median(get_within_sample_range(df_results$directional_bf))
mwsr_p_effect <- median(get_within_sample_range(df_results$p_effect))

mwsr_iqr_savage_dickey_bf <- IQR(get_within_sample_range(df_results$savage_dickey_bf))
mwsr_iqr_directional_bf <- IQR(get_within_sample_range(df_results$directional_bf))
mwsr_iqr_p_effect <- IQR(get_within_sample_range(df_results$p_effect))


# Prior sensitivity (ps; proportion of samples in which a method came to diverging results)
df_results$fp_directional_bf_sim <- df_results$fp_directional_bf_sim_pos + df_results$fp_directional_bf_sim_neg
df_results$fp_p_effect_sim <- df_results$fp_p_effect_sim_pos + df_results$fp_p_effect_sim_neg
df_results$fp_hdi_sim <- df_results$fp_hdi_sim_pos + df_results$fp_hdi_sim_neg

get_prior_sensitivity <- function(column){
  counter <- 0
  for (i in seq(from = 1, by = 8, length.out = length(column)/8)){
    if (all(column[i] == column[seq(i, i+7)]) == FALSE){
      counter <- counter + 1
    }
  }
  proportion <- counter/(length(column)/8)
  return(proportion)
}

ps_savage_dickey_bf_sim <- get_prior_sensitivity(df_results$fp_savage_dickey_bf_sim)
ps_savage_dickey_bf_sim_0_2 <-get_prior_sensitivity(df_results$fp_savage_dickey_bf_sim[df_results$s_log_k_sd == 0.2])
ps_savage_dickey_bf_sim_0_51 <- get_prior_sensitivity(df_results$fp_savage_dickey_bf_sim[df_results$s_log_k_sd == 0.51])
ps_savage_dickey_bf_sim_0_81 <- get_prior_sensitivity(df_results$fp_savage_dickey_bf_sim[df_results$s_log_k_sd == 0.81])

ps_directional_bf_sim <- get_prior_sensitivity(df_results$fp_directional_bf_sim)
ps_directional_bf_sim_0_2 <- get_prior_sensitivity(df_results$fp_directional_bf_sim[df_results$s_log_k_sd == 0.2])
ps_directional_bf_sim_0_51 <- get_prior_sensitivity(df_results$fp_directional_bf_sim[df_results$s_log_k_sd == 0.51])
ps_directional_bf_sim_0_81 <- get_prior_sensitivity(df_results$fp_directional_bf_sim[df_results$s_log_k_sd == 0.81])

ps_hdi_sim <- get_prior_sensitivity(df_results$fp_hdi_sim)
ps_hdi_sim_0_2 <- get_prior_sensitivity(df_results$fp_hdi_sim[df_results$s_log_k_sd == 0.2])
ps_hdi_sim_0_51 <- get_prior_sensitivity(df_results$fp_hdi_sim[df_results$s_log_k_sd == 0.51])
ps_hdi_sim_0_81 <- get_prior_sensitivity(df_results$fp_hdi_sim[df_results$s_log_k_sd == 0.81])

ps_p_effect_sim <- get_prior_sensitivity(df_results$fp_p_effect_sim)
ps_p_effect_sim_0_2 <- get_prior_sensitivity(df_results$fp_p_effect_sim[df_results$s_log_k_sd == 0.2])
ps_p_effect_sim_0_51 <- get_prior_sensitivity(df_results$fp_p_effect_sim[df_results$s_log_k_sd == 0.51])
ps_p_effect_sim_0_81 <- get_prior_sensitivity(df_results$fp_p_effect_sim[df_results$s_log_k_sd == 0.81])


# Formatting functions
remove_leading_zero <- function(value){
  value <- format(round(value, 2), nsmall = 2)
  value <- gsub("0\\.", ".", value)
  return(value)
}

remove_leading_zero_up <- function(value){
  value <- format(ceiling(value * 10^2) / 10^2, nsmall = 2)
  value <- gsub("0\\.", ".", value)
  return(value)
}

remove_leading_zero_down <- function(value){
  value <- format(floor(value * 10^2) / 10^2, nsmall = 2)
  value <- gsub("0\\.", ".", value)
  return(value)
}

proportion_to_percentage <- function(value){
  value <- format(round(value*100, 2), nsmall = 2)
  return(value)
}

proportion_to_percentage_up <- function(value){
  value <- format(ceiling((value*100) * 10^2) / 10^2, nsmall = 2)
  return(value)
}

round_and_format <- function(value){
  value <- format(round(value, 2), nsmall = 2)
  return(value)
}

round_and_format_up <- function(value){
  value <-  format(ceiling(value * 10^2) / 10^2, nsmall = 2)
  return(value)
}

round_and_format_down <- function(value){
  value <-  format(floor(value * 10^2) / 10^2, nsmall = 2)
  return(value)
}

```

`\documentclass[man,hidelinks,12pt,floatsintext]{apa7}`
`\usepackage[american]{babel}`
`\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}`
`\usepackage{float}`
`\usepackage{amsmath}`
`\usepackage{graphicx}`
`\usepackage{hyperref}`
`\usepackage{subcaption}`
`\usepackage{setspace}`
`\usepackage[font=doublespacing]{caption}`
`\usepackage{tabularx}`
`\DeclareLanguageMapping{american}{american-apa}`

`\title{Simulation-Based Comparison of Bayesian Inference Methods in Hierarchical Bayesian Computational Models}`
`\authorsnames[{1,2},1]{Simon Michalski, Jan Peters}`
`\authorsaffiliations{{Department of Psychology, Biological Psychology, University of Cologne, Cologne, Germany}, {Section of Medical Psychology, Department of Psychiatry and Psychotherapy, Faculty of Medicine, University of Bonn, Bonn, Germany}}`
`\shorttitle{}`

`\begin{document}`
`\maketitle`
`\section{Results}` 
`\setcounter{table}{2}`
`\setcounter{figure}{2}`
Our analysis proceeded in several steps. First, we carried out parameter recovery analyses for subject-level and group-level parameters. Next, we examined the impact of the different inference procedures on false positive rates across various decision thresholds. We then computed simulation-based decision thresholds for single and multiple tests that limit false positives to a maximum of 5`\%`. In a final step, we checked the degree to which inference methods arrived at diverging conclusions for the same models and compared the prior sensitivity of inference methods.
<br>
`\subsection{Parameter Recovery Analyses}`  
The ability of a computational model to accurately recover know parameter values is a requirement for any model (Wilson `\&` Collins, 2019). Here, we carried out parameter recovery analyses for both subject-level parameters and group level means, focusing on the condition-effect parameter `\textit{s}\textsubscript{log(\textit{k})}`. For subject-level parameter recovery, we used the medians of the subject-level posterior distributions, and checked the degree to which these estimates correlated with the true parameter values used for data simulation. The median correlation between true and estimated subject-level `\textit{s}\textsubscript{log(\textit{k})}` parameters was `\textit{r}` = `r remove_leading_zero(median_cor_s_log_k_0_2)` (`\textit{IQR}` = `r remove_leading_zero(iqr_cor_s_log_k_0_2)`) for models fitted to data sets with a small population effect standard deviation (see Table 1), `\textit{r}` = `r remove_leading_zero(median_cor_s_log_k_0_51)` (`\textit{IQR}` = `r remove_leading_zero(iqr_cor_s_log_k_0_51)`) for models fitted to data sets from the medium population effect standard deviation (see Table 1), and `\textit{r}` = `r remove_leading_zero(median_cor_s_log_k_0_81)` (`\textit{IQR}` = `r remove_leading_zero(iqr_cor_s_log_k_0_81)`) for models fitted to data sets from the large population effect standard deviation (see Table 1). The median correlation across all population effect standard deviations was `\textit{r}` = `r remove_leading_zero(median_cor_s_log_k_overall)` (`\textit{IQR}` = `r remove_leading_zero(iqr_cor_s_log_k_overall)`). For subject-level log(`\textit{k}`), parameter recovery was also high with a correlation `\textit{r}` = `r remove_leading_zero(median_cor_log_k_overall)` (`\textit{IQR}` = `r remove_leading_zero(iqr_cor_log_k_overall)`). Together, these analyses confirm adequate parameter recovery of `\textit{s}\textsubscript{log(\textit{k})}`, and confirm reduced recovery for lower population effect standard deviations, similar to previous results for test-retest correlations in hierarchical Bayesian models (Scheibehenne `\&` Pachur, 2015).
`\\`  
In order to quantify the degree of partial pooling, the relative shrinkage of subject-level parameter variability was computed for each model (Kruschke, 2015). On average, the standard deviation of true subject-level `\textit{s}\textsubscript{log(\textit{k})}` parameters was reduced by `r proportion_to_percentage(mean_shrinkage_overall)``\%` (`\textit{SD}` = `r proportion_to_percentage(sd_shrinkage_overall)`). Shrinkage of the standard deviations of subject-level parameters was stronger, the lower the standard deviation of the sampling distribution was (Table 3). See Figure 3a for the absolute ratios of the standard deviations of true and estimated subject-level `\textit{s}\textsubscript{log(\textit{k})}` parameters per model. Unsurprisingly, shrinkage was similar across all variations of the group-level mean prior of `\textit{s}\textsubscript{log(\textit{k})}`. Together, this confirms that shrinkage highly depends on between-subjects variation and can be negligible, when the population variability is high.
<br>
```
\begin{table}[H]
    \caption{Shrinkage of the standard deviations of subject-level parameters}
    \label{tab:shrinkage}
    \begin{tabularx}{\textwidth}{lXXXXXX}
    \midrule
    \multicolumn{1}{c}{Sampling distribution} & \multicolumn{2}{c}{\textit{SD} true parameters} & \multicolumn{2}{c}{\textit{SD} estimates} & \multicolumn{2}{c}{\textit{SD} reduction (\%)}\\
    \cmidrule{2-7}
    \multicolumn{1}{c}{} & \multicolumn{1}{c}{\textit{M}} & \multicolumn{1}{c}{\textit{SD}} & \multicolumn{1}{c}{\textit{M}} & \multicolumn{1}{c}{\textit{SD}} & \multicolumn{1}{c}{\textit{M}} & \multicolumn{1}{c}{\textit{SD}}\\
    \midrule
    \textit{s}\textsubscript{log(\textit{k})} $\sim \mathcal{N}$(0,0.20) & \centering `r round_and_format(mean_sd_true_0_2)` & \centering `r round_and_format(sd_sd_true_0_2)` & \centering `r round_and_format(mean_sd_estimates_0_2)` & \centering `r round_and_format(sd_sd_estimates_0_2)` & \centering `r proportion_to_percentage(mean_shrinkage_0_2)` & \centering `r  proportion_to_percentage(sd_shrinkage_0_2)`\arraybackslash\\
    \textit{s}\textsubscript{log(\textit{k})} $\sim \mathcal{N}$(0,0.51) & \centering `r round_and_format(mean_sd_true_0_51)` & \centering `r round_and_format(sd_sd_true_0_51)` & \centering `r round_and_format(mean_sd_estimates_0_51)` & \centering `r round_and_format(sd_sd_estimates_0_51)` & \centering `r proportion_to_percentage(mean_shrinkage_0_51)` & \centering `r proportion_to_percentage(sd_shrinkage_0_51)`\arraybackslash\\
    \textit{s}\textsubscript{log(\textit{k})} $\sim \mathcal{N}$(0,0.81) & \centering `r round_and_format(mean_sd_true_0_81)` & \centering `r round_and_format(sd_sd_true_0_81)` & \centering `r round_and_format(mean_sd_estimates_0_81)` & \centering `r  round_and_format(sd_sd_estimates_0_81)` & \centering `r proportion_to_percentage(mean_shrinkage_0_81)` & \centering `r proportion_to_percentage(sd_shrinkage_0_81)`\arraybackslash\\
    log(\textit{k}) $\sim \mathcal{N}$(-4.79,1.02) & \centering `r round_and_format(mean_sd_true_overall_log_k)` & \centering `r round_and_format(sd_sd_true_overall_log_k)` & \centering `r round_and_format(mean_sd_estimates_overall_log_k)` & \centering `r  round_and_format(sd_sd_estimates_overall_log_k)` & \centering `r proportion_to_percentage(mean_shrinkage_overall_log_k)` & \centering `r  proportion_to_percentage(sd_shrinkage_overall_log_k)`\arraybackslash\\
    \midrule
    \end{tabularx}
\end{table}
```
<br>
```
\begin{figure} [H]
    \centering
    \caption{}
    \label{fig:recovery}
    \includegraphics[width=1\linewidth]{figures/fig3.eps}
    \caption*{\textnormal{Recovery of individual and group-level shift parameters. (a) Ratios of the standard deviations of true and estimated subject-level parameters per model. The standard deviations of true subject-level parameters of models below the line were shrunk. Note that absolute shrinkage was comparable between population effect standard deviations, but relative shrinkage was higher, the lower the population effect standard deviation was (Table 3). (b) Decrease of absolute group-level mean estimates toward zero with informative priors. Note that with wider priors, estimates of the group-level mean could slightly diverge from their true mean in both directions due to the added noise in the data, which resulted in a slightly higher range of estimated group-level means. Hence, when comparing absolute means, mean absolute estimates of the group-level mean were descriptively higher than mean absolute true group-level means with wider priors. Prior standard deviations are plotted on a logarithmic scale. Omitted tick labels are 1.50 and 2.00.}}
\end{figure}
```
<br>
Group-level parameter recovery focused on the medians of the respective group level posterior distributions. As we simulated null effects, the mean true group-level mean of `\textit{s}\textsubscript{log(\textit{k})}` parameters were generally close to zero (small population `\textit{SD}`: `\textit{M}` = `r round_and_format(mean_true_group_level_mean_0_2)` (`\textit{SD}` = `r round_and_format(sd_true_group_level_mean_0_2)`), medium population `\textit{SD}`: `\textit{M}` = `r round_and_format(mean_true_group_level_mean_0_51)` (`\textit{SD}` = `r round_and_format(sd_true_group_level_mean_0_51)`), large population `\textit{SD}`: `\textit{M}` = `r round_and_format(mean_true_group_level_mean_0_81)` (`\textit{SD}` = `r round_and_format(sd_true_group_level_mean_0_81)`)). The mean true group-level mean of log(`\textit{k}`) parameters was `r round_and_format(mean_true_group_level_mean_log_k)` (`\textit{SD}` = `r round_and_format(sd_true_group_level_mean_log_k)`). The mean estimates of the group-level mean of `\textit{s}\textsubscript{log(\textit{k})}` were also generally close to zero (small population `\textit{SD}`: `\textit{M}` = `r round_and_format(mean_group_level_mean_0_2)` (`\textit{SD}` = `r round_and_format(sd_group_level_mean_0_2)`), medium population `\textit{SD}`: `\textit{M}` = `r round_and_format(mean_group_level_mean_0_51)` (`\textit{SD}` = `r round_and_format(sd_group_level_mean_0_51)`), large population `\textit{SD}`: `\textit{M}` = `r round_and_format(mean_group_level_mean_0_81)` (`\textit{SD}` = `r round_and_format(sd_group_level_mean_0_81)`)). The mean estimate of the group-level mean of log(`\textit{k}`) was `r round_and_format(mean_group_level_mean_log_k)` (`\textit{SD}` = `r round_and_format(sd_group_level_mean_log_k)`).
`\\`
To examine how the SDs of priors affect estimated means, we computed the mean absolute estimates of the group-level mean of `\textit{s}\textsubscript{log(\textit{k})}` per population effect and prior standard deviation (see Fig. `\hyperref[fig:recovery]{\ref{fig:recovery}b}`). Here, higher values indicate larger deviations from zero across models, which we observed for increasingly wider priors (Fig. `\hyperref[fig:recovery]{\ref{fig:recovery}b}`). As expected, a wider (more vague) prior of the group-level mean allowed the estimate to vary more with the data, thus moving it closer to the true mean. In contrast, a stronger (more precise) prior drew the estimate closer to zero. 
`\\`  
Since we used uniform priors for the group level `\textit{SD}` of `\textit{s}\textsubscript{log(\textit{k})}` for all models, the estimate of the group-level `\textit{SD}` was primarily driven by the data. Therefore, the mean estimates of the group-level standard deviation of `\textit{s}\textsubscript{log(\textit{k})}` were similar to the respective population effect standard deviations used for simulation (small population `\textit{SD}`: `\textit{M}` = `r round_and_format(mean_group_level_sd_0_2)` (`\textit{SD}` = `r round_and_format(sd_group_level_sd_0_2)`), medium population `\textit{SD}`: `\textit{M}` = `r round_and_format(mean_group_level_sd_0_51)` (`\textit{SD}` = `r round_and_format(sd_group_level_sd_0_51)`), large population `\textit{SD}`: `\textit{M}` = `r round_and_format(mean_group_level_sd_0_81)` (`\textit{SD}` = `r round_and_format(sd_group_level_sd_0_81)`)). The mean estimate of the group-level standard deviation of log(`\textit{k}`) was `r round_and_format(mean_group_level_sd_log_k)` (`\textit{SD}` = `r round_and_format(sd_group_level_sd_log_k)`). Together, these analyses confirm that group-level parameters could be accurately recovered using our simulation and model estimation approach. As expected, narrower priors increasingly drew the group-level means towards zero (i.e. the mean of the prior).
<br>
`\subsection{Inference Methods}`
In a first step, we examined the results of the different inference procedures. Across all models, that is across all prior widths, Savage-Dickey BFs ranged from `r round_and_format(min(df_results$savage_dickey_bf))` to `r round_and_format(max(df_results$savage_dickey_bf))` (`\textit{Mdn}` = `r round_and_format(median(df_results$savage_dickey_bf))`, `\textit{IQR}` = `r round_and_format(IQR(df_results$savage_dickey_bf))`), dBFs ranged from `r round_and_format(min(df_results$directional_bf))` to `r round_and_format(max(df_results$directional_bf))` (`\textit{Mdn}` = `r round_and_format(median(df_results$directional_bf))`, `\textit{IQR}` = `r round_and_format(IQR(df_results$directional_bf))`), and P(effect > 0) proportions ranged from `r remove_leading_zero(min(df_results$p_effect))` to `r remove_leading_zero(max(df_results$p_effect))` (`\textit{Mdn}` = `r remove_leading_zero(median(df_results$p_effect))`, `\textit{IQR}` = `r remove_leading_zero(IQR(df_results$p_effect))`). Boxplots of Savage-Dickey BFs, dBFs, and P(effect > 0) proportions per prior and population effect standard deviation are shown in Figure 4. As expected, the Savage-Dickey BF, but not inferences that depend solely on the posterior, were strongly modulated by prior width. With increasing prior width, the median Savage-Dickey BF decreased (Figure 4a), whereas the median dBF and median P(effect > 0) remained largely unaffected (Figure 4b,c).
<br>
```
\begin{figure} [H]
    \centering
    \caption{}
    \label{fig:values}
    \includegraphics[width=1\linewidth]{figures/fig4.eps}
    \caption*{\textnormal{(a) Savage-Dickey BFs, (b) dBFs, and (c) P(effect > 0) proportions of all models. The horizontal line within each box shows the median, the box spans from the first to the third quartile, the lower whisker extends to the lowest value within 1.5*\textit{IQR} below the first quartile, the upper whisker extends to the highest value within 1.5*\textit{IQR} above the third quartile, outliers are displayed as dots, which are all data points outside the lower and upper whisker. Savage-Dickey BFs and dBFs are plotted on a logarithmic scale.}}
\end{figure}
```
<br>
`\subsection{False Positive Results}`
We next used our simulations to examine false positive rates for different commonly used inference procedures, as well as to test the degree to which these depend on prior width and the population SD in condition effects. Recall that all simulations were based on condition effects that were centered at zero, but different in variance (see Table 1). The proportions of false positive results of every decision rule were calculated for all combinations of the prior and population effect SD (Fig. 5), per prior width averaged over all population effect SDs (Table 4), and overall. With a decision threshold of 3, `r proportion_to_percentage(fp_savage_dickey_bf_3)``\%` of Savage-Dickey BFs were false positives (Fig. 5a). For dBFs, false positive rates only reached levels <5`\%` with thresholds of 1/30 and 30 and stricter (see Fig 5e-h). For the HDI against zero decision rule, false positive rates only reached <5`\%` with the 95`\%` HDI or stricter (see Fig. 5i-l). For P(effect > 0), false positive rates were `r proportion_to_percentage(fp_p_effect_95)``\%` for criterion on 95`\%` of posterior samples beyond zero, `r proportion_to_percentage(fp_p_effect_975)``\%` for 97.5`\%` of samples, and `r proportion_to_percentage(fp_p_effect_99)``\%` for 99`\%` of samples. Unsurprisingly, stricter decision thresholds lowered the proportion of false positive results for each inference method. False positive rates increased with increasing prior width for the dBF, P(effect > 0), and the HDI against zero decision rules, whereas this was not the case for Savage-Dickey BFs. With wider priors and stricter thresholds, false positive rates tended to be descriptively higher for data sets from the highest population effect standard deviation (see the blue lines in Figure 5).
<br>
```
\begin{figure} [H]
    \centering
    \caption{}
    \label{fig:false_positves}
    \includegraphics[width=1\linewidth]{figures/fig5.eps}
    \caption*{\textnormal{Proportions of false positive Savage-Dickey BFs (a), P(effect > 0) proportions (b-d), dBFs (e-h), and HDI decisions (i-l) with conventional decision thresholds. Dashed lines indicate a proportion of 5\% false positive results. Prior standard deviations are plotted on a logarithmic scale. Omitted tick labels are 0.10, 0.50, 1.50, and 2.00.}}
\end{figure}
```
<br>
```
\begin{table}[H]
    \caption{Proportions of false positive results for various decision thresholds per prior standard deviation. Results are averaged across three levels of population effect SDs}
    \label{tab:false_positives_prior}
    \begin{tabularx}{\textwidth}{lXXXXXXXX}
    \midrule
    \multicolumn{1}{c}{Decision rule} & \multicolumn{8}{c}{False positive results (\% of models) per prior \textit{SD} (\%)}\\
    \cmidrule{2-9}
    \multicolumn{1}{c}{} & \multicolumn{1}{c}{0.05} & \multicolumn{1}{c}{0.10} & \multicolumn{1}{c}{0.20} & \multicolumn{1}{c}{0.50} & \multicolumn{1}{c}{1.00} & \multicolumn{1}{c}{1.50} & \multicolumn{1}{c}{2.00} & \multicolumn{1}{c}{2.50}\\
    \midrule
    Savage-Dickey BF\textsubscript{10} > 3 & \centering `r fp_savage_dickey_bf_3_priors[1]` & \centering `r fp_savage_dickey_bf_3_priors[2]` & \centering `r fp_savage_dickey_bf_3_priors[3]` & \centering `r fp_savage_dickey_bf_3_priors[4]` & \centering `r fp_savage_dickey_bf_3_priors[5]` & \centering `r fp_savage_dickey_bf_3_priors[6]` & \centering `r fp_savage_dickey_bf_3_priors[7]` & \centering `r fp_savage_dickey_bf_3_priors[8]`\arraybackslash\\
    dBF\textsubscript{+$-$} [1/3, 3] & \centering `r fp_directional_bf_3_priors[1]` & \centering `r fp_directional_bf_3_priors[2]` & \centering `r fp_directional_bf_3_priors[3]` & \centering `r fp_directional_bf_3_priors[4]` & \centering `r fp_directional_bf_3_priors[5]` & \centering `r fp_directional_bf_3_priors[6]` & \centering `r fp_directional_bf_3_priors[7]` & \centering `r fp_directional_bf_3_priors[8]`\arraybackslash\\
    dBF\textsubscript{+$-$} [1/10, 10] & \centering `r fp_directional_bf_10_priors[1]` & \centering `r fp_directional_bf_10_priors[2]` & \centering `r fp_directional_bf_10_priors[3]` & \centering `r fp_directional_bf_10_priors[4]` & \centering `r fp_directional_bf_10_priors[5]` & \centering `r fp_directional_bf_10_priors[6]` & \centering `r fp_directional_bf_10_priors[7]` & \centering `r fp_directional_bf_10_priors[8]`\arraybackslash\\
    dBF\textsubscript{+$-$} [1/30, 30] & \centering `r fp_directional_bf_30_priors[1]` & \centering `r fp_directional_bf_30_priors[2]` & \centering `r fp_directional_bf_30_priors[3]` & \centering `r fp_directional_bf_30_priors[4]` & \centering `r fp_directional_bf_30_priors[5]` & \centering `r fp_directional_bf_30_priors[6]` & \centering `r fp_directional_bf_30_priors[7]` & \centering `r fp_directional_bf_30_priors[8]`\arraybackslash\\
    dBF\textsubscript{+$-$} [1/100, 100] & \centering `r fp_directional_bf_100_priors[1]` & \centering `r fp_directional_bf_100_priors[2]` & \centering `r fp_directional_bf_100_priors[3]` & \centering `r fp_directional_bf_100_priors[4]` & \centering `r fp_directional_bf_100_priors[5]` & \centering `r fp_directional_bf_100_priors[6]` & \centering `r fp_directional_bf_100_priors[7]` & \centering `r fp_directional_bf_100_priors[8]`\arraybackslash\\
    P(effect > 0) [.05, .95] & \centering `r fp_p_effect_95_priors[1]` & \centering `r fp_p_effect_95_priors[2]` & \centering `r fp_p_effect_95_priors[3]` & \centering `r fp_p_effect_95_priors[4]` & \centering `r fp_p_effect_95_priors[5]` & \centering `r fp_p_effect_95_priors[6]` & \centering `r fp_p_effect_95_priors[7]` & \centering `r fp_p_effect_95_priors[8]`\arraybackslash\\
    P(effect > 0) [.025, .975] & \centering `r fp_p_effect_975_priors[1]` & \centering `r fp_p_effect_975_priors[2]` & \centering `r fp_p_effect_975_priors[3]` & \centering `r fp_p_effect_975_priors[4]` & \centering `r fp_p_effect_975_priors[5]` & \centering `r fp_p_effect_975_priors[6]` & \centering `r fp_p_effect_975_priors[7]` & \centering `r fp_p_effect_975_priors[8]`\arraybackslash\\
    P(effect > 0) [.01, .99] & \centering `r fp_p_effect_99_priors[1]` & \centering `r fp_p_effect_99_priors[2]` & \centering `r fp_p_effect_99_priors[3]` & \centering `r fp_p_effect_99_priors[4]` & \centering `r fp_p_effect_99_priors[5]` & \centering `r fp_p_effect_99_priors[6]` & \centering `r fp_p_effect_99_priors[7]` & \centering `r fp_p_effect_99_priors[8]`\arraybackslash\\
    80\% HDI & \centering `r fp_hdi_80_priors[1]` & \centering `r fp_hdi_80_priors[2]` & \centering `r fp_hdi_80_priors[3]` & \centering `r fp_hdi_80_priors[4]` & \centering `r fp_hdi_80_priors[5]` & \centering `r fp_hdi_80_priors[6]` & \centering `r fp_hdi_80_priors[7]` & \centering `r fp_hdi_80_priors[8]`\arraybackslash\\
    90\% HDI & \centering `r fp_hdi_90_priors[1]` & \centering `r fp_hdi_90_priors[2]` & \centering `r fp_hdi_90_priors[3]` & \centering `r fp_hdi_90_priors[4]` & \centering `r fp_hdi_90_priors[5]` & \centering `r fp_hdi_90_priors[6]` & \centering `r fp_hdi_90_priors[7]` & \centering `r fp_hdi_90_priors[8]`\arraybackslash\\
    95\% HDI & \centering `r fp_hdi_95_priors[1]` & \centering `r fp_hdi_95_priors[2]` & \centering `r fp_hdi_95_priors[3]` & \centering `r fp_hdi_95_priors[4]` & \centering `r fp_hdi_95_priors[5]` & \centering `r fp_hdi_95_priors[6]` & \centering `r fp_hdi_95_priors[7]` & \centering `r fp_hdi_95_priors[8]`\arraybackslash\\
    99\% HDI & \centering `r fp_hdi_99_priors[1]` & \centering `r fp_hdi_99_priors[2]` & \centering `r fp_hdi_99_priors[3]` & \centering `r fp_hdi_99_priors[4]` & \centering `r fp_hdi_99_priors[5]` & \centering `r fp_hdi_99_priors[6]` & \centering `r fp_hdi_99_priors[7]` & \centering `r fp_hdi_99_priors[8]`\arraybackslash\\
    \midrule
    \end{tabularx}
\end{table}
```
<br>
`\subsection{Simulation-Based Decision Thresholds}`  
We next determined how thresholds in the different inference procedures would need to be adjusted in order to limit false positives to a maximum of 5`\%`. Corresponding thresholds were computed for each inference method for all combinations of the prior and population effect standard deviation (Fig. 6). Upper thresholds were rounded up, lower thresholds were rounded down, and directional thresholds were set to 2.5`\%` false positive results on each side. Resulting adjusted thresholds per prior standard deviation, and averaged over all three population effect standard deviations, are listed in  Table 5. For example, a dBF of around 30 (or 1/30 for a negative effect) effectively keeps the false positive rate below 5`\%` (averaged across population effect `\textit{SD}`s).
<br>
```
\begin{figure} [H]
    \centering
    \caption{}
    \label{fig:sim_thres}
    \includegraphics[width=1\linewidth]{figures/fig6.eps}
    \caption*{\textnormal{Simulation-based decision thresholds for each inference method. Prior standard deviations are plotted on a logarithmic scale. Omitted tick labels are 0.10, 0.50, 1.50, and 2.00.}}
\end{figure}
```
<br>
```
\begin{table}[H]
    \caption{Simulation-based decision thresholds per prior standard deviation.}
    \label{tab:sim_thres_prior}
    \begin{tabularx}{\textwidth}{lXXXXXXXX}
    \midrule
    \multicolumn{1}{c}{Inference method} & \multicolumn{8}{c}{Simulation-based decision threshold per prior \textit{SD}}\\
    \cmidrule{2-9}
    \multicolumn{1}{c}{} & \multicolumn{1}{c}{0.05} & \multicolumn{1}{c}{0.10} & \multicolumn{1}{c}{0.20} & \multicolumn{1}{c}{0.50} & \multicolumn{1}{c}{1.00} & \multicolumn{1}{c}{1.50} & \multicolumn{1}{c}{2.00} & \multicolumn{1}{c}{2.50}\\
    \midrule
    Savage-Dickey BF\textsubscript{10} & \centering `r round_and_format_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 0.05,"savage_dickey_bf"])` & \centering `r round_and_format_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 0.1,"savage_dickey_bf"])` & \centering `r round_and_format_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 0.2,"savage_dickey_bf"])` & \centering `r round_and_format_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 0.5,"savage_dickey_bf"])` & \centering `r round_and_format_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 1,"savage_dickey_bf"])` & \centering `r round_and_format_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 1.5,"savage_dickey_bf"])` & \centering `r round_and_format_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 2,"savage_dickey_bf"])` & \centering `r round_and_format_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 2.5,"savage_dickey_bf"])`\arraybackslash\\
    dBF\textsubscript{+$-$} (upper) & \centering `r round_and_format_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 0.05,"directional_bf_upper"])` & \centering `r round_and_format_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 0.1,"directional_bf_upper"])` & \centering `r round_and_format_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 0.2,"directional_bf_upper"])` & \centering `r round_and_format_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 0.5,"directional_bf_upper"])` & \centering `r round_and_format_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 1,"directional_bf_upper"])` & \centering `r round_and_format_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 1.5,"directional_bf_upper"])` & \centering `r round_and_format_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 2,"directional_bf_upper"])` & \centering `r round_and_format_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 2.5,"directional_bf_upper"])`\arraybackslash\\
    dBF\textsubscript{+$-$} (lower) & \centering 1/`r round_and_format_up(1/df_sim_thres_prior[df_sim_thres_prior$prior_sd == 0.05,"directional_bf_lower"])` & \centering 1/`r round_and_format_up(1/df_sim_thres_prior[df_sim_thres_prior$prior_sd == 0.1,"directional_bf_lower"])` & \centering 1/`r round_and_format_up(1/df_sim_thres_prior[df_sim_thres_prior$prior_sd == 0.2,"directional_bf_lower"])` & \centering 1/`r round_and_format_up(1/df_sim_thres_prior[df_sim_thres_prior$prior_sd == 0.5,"directional_bf_lower"])` & \centering 1/`r round_and_format_up(1/df_sim_thres_prior[df_sim_thres_prior$prior_sd == 1,"directional_bf_lower"])` & \centering 1/`r round_and_format_up(1/df_sim_thres_prior[df_sim_thres_prior$prior_sd == 1.5,"directional_bf_lower"])` & \centering 1/`r round_and_format_up(1/df_sim_thres_prior[df_sim_thres_prior$prior_sd == 2,"directional_bf_lower"])` & \centering 1/`r round_and_format_up(1/df_sim_thres_prior[df_sim_thres_prior$prior_sd == 2.5,"directional_bf_lower"])`\arraybackslash\\
    P(effect > 0) (upper) & \centering `r remove_leading_zero_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 0.05,"p_effect_upper"])` & \centering `r remove_leading_zero_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 0.1,"p_effect_upper"])` & \centering `r remove_leading_zero_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 0.2,"p_effect_upper"])` & \centering `r remove_leading_zero_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 0.5,"p_effect_upper"])` & \centering `r remove_leading_zero_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 1,"p_effect_upper"])` & \centering `r remove_leading_zero_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 1.5,"p_effect_upper"])` & \centering `r remove_leading_zero_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 2,"p_effect_upper"])` & \centering `r remove_leading_zero_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 2.5,"p_effect_upper"])`\arraybackslash\\
    P(effect > 0) (lower) & \centering `r remove_leading_zero_down(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 0.05,"p_effect_lower"])` & \centering `r remove_leading_zero_down(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 0.1,"p_effect_lower"])` & \centering `r remove_leading_zero_down(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 0.2,"p_effect_lower"])` & \centering `r remove_leading_zero_down(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 0.5,"p_effect_lower"])` & \centering `r remove_leading_zero_down(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 1,"p_effect_lower"])` & \centering `r remove_leading_zero_down(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 1.5,"p_effect_lower"])` & \centering `r remove_leading_zero_down(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 2,"p_effect_lower"])` & \centering `r remove_leading_zero_down(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 2.5,"p_effect_lower"])`\arraybackslash\\
    HDI width (\%) & \centering `r proportion_to_percentage_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 0.05,"hdi"])` & \centering `r proportion_to_percentage_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 0.1,"hdi"])` & \centering `r proportion_to_percentage_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 0.2,"hdi"])` & \centering `r proportion_to_percentage_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 0.5,"hdi"])` & \centering `r proportion_to_percentage_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 1,"hdi"])` & \centering `r proportion_to_percentage_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 1.5,"hdi"])` & \centering `r proportion_to_percentage_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 2,"hdi"])` & \centering `r proportion_to_percentage_up(df_sim_thres_prior[df_sim_thres_prior$prior_sd == 2.5,"hdi"])`\arraybackslash\\
    \midrule
    \end{tabularx}
\end{table}
```
<br>
`\subsubsection{Prior Sensitivity}`  
To compare how inference methods were influenced by different prior widths (when modeling the same data), we calculated the range of Savage-Dickey BFs, dBFs, and P(effect > 0) proportions of the eight models of each data set. Prior sensitivity tended to be higher for Savage-Dickey BFs (`\textit{Mdn}`: `r round_and_format(mwsr_savage_dickey_bf)`, `\textit{IQR}` = `r round_and_format(mwsr_iqr_savage_dickey_bf)`) than for dBFs (`\textit{Mdn}`: `r round_and_format(mwsr_directional_bf)`, `\textit{IQR}` = `r round_and_format(mwsr_iqr_directional_bf)`) and P(effect > 0) proportions (`\textit{Mdn}` = `r round_and_format(mwsr_p_effect)`, `\textit{IQR}` = `r round_and_format(mwsr_iqr_p_effect)`).  
`\\`  
To assess the prior sensitivity of the inference methods in terms of the hypothesis decision, the respective proportions of data sets for which the decisions of an inference method diverged across prior widths were calculated. We used simulation-based decision thresholds over all prior and population effect standard deviations (see Table 5) to keep the total number of false positive results comparable. Diverging results due to different priors were around or below 10`\%` for the different inference methods (Savage-Dickey BF: `r proportion_to_percentage(ps_savage_dickey_bf_sim)``\%`, dBFs: `r proportion_to_percentage(ps_directional_bf_sim)``\%`, HDI: `r proportion_to_percentage(ps_hdi_sim)``\%`, P(effect > 0): `r proportion_to_percentage(ps_p_effect_sim)``\%`). See Table 6 for the prior sensitivities per population effect standard deviations. The prior sensitivity of the Savage-Dickey BF slightly decreased descriptively with increasing standard deviation of the population effect, whereas the prior sensitivity of the dBF, P(effect > 0), and the HDI against zero decision rule descriptively increased with increasing population effect standard deviation. Together, these analyses confirm that the prior sensitivity of the Savage-Dickey BF was descriptively higher than the prior sensitivity of methods that solely depend on the posterior. Nevertheless, the prior sensitivity was small for all methods.
<br>
```
\begin{table}[H]
    \caption{Prior sensitivity of each inference method per population effect standard deviation}
    \label{tab:prior_sensitivity}
    \begin{tabularx}{\textwidth}{lXXX}
    \midrule
    \multicolumn{1}{c}{Inference method} & \multicolumn{3}{c}{Data sets with diverging results per population effect \textit{SD} (\%)}\\
    \cmidrule{2-4}
    \multicolumn{1}{c}{} & \multicolumn{1}{c}{0.20} & \multicolumn{1}{c}{0.51} & \multicolumn{1}{c}{0.81}\\
    \midrule
    Savage-Dickey BF\textsubscript{10} & \centering `r proportion_to_percentage(ps_savage_dickey_bf_sim_0_2)` & \centering `r proportion_to_percentage(ps_savage_dickey_bf_sim_0_51)` & \centering `r proportion_to_percentage(ps_savage_dickey_bf_sim_0_81)`\arraybackslash\\
    dBF\textsubscript{+$-$} & \centering `r proportion_to_percentage(ps_directional_bf_sim_0_2)` & \centering `r proportion_to_percentage(ps_directional_bf_sim_0_51)` & \centering `r proportion_to_percentage(ps_directional_bf_sim_0_81)`\arraybackslash\\
    95\% HDI & \centering `r proportion_to_percentage(ps_hdi_sim_0_2)` & \centering `r proportion_to_percentage(ps_hdi_sim_0_51)` & \centering `r proportion_to_percentage(ps_hdi_sim_0_81)`\arraybackslash\\
    P(effect > 0) & \centering `r proportion_to_percentage(ps_p_effect_sim_0_2)` & \centering `r proportion_to_percentage(ps_p_effect_sim_0_51)` & \centering `r proportion_to_percentage(ps_p_effect_sim_0_81)`\arraybackslash\\
    \midrule
    \end{tabularx}
\end{table}
```
<br>
`\subsubsection{Adjustments for Multiple Testing}`
We next estimated potential adjustments required when conducting multiple hypothesis tests to accept or reject a disjunctive joint hypothesis. To this end, decision thresholds (averaged over all prior and population effect standard deviations), were adjusted under the assumption of statistically independent parameters by tightening the respective threshold percentiles and lowering the percentage of HDIs that exclude zero with increasing number of tests (Fig. 7). For example, for the Savage-Dickey BF (Fig. 7a) and dBF (Fig. 7c), there were largely linear associations between adjusted BFs and number of conducted tests. A decision rule with a 97`\%` HDI width (Fig. 7d), for example, largely controls the false positive rate for three independent comparisons.
```
\begin{figure} [H]
    \centering
    \caption{}
    \label{fig:thresholds_multiple_test}
    \includegraphics[width=1\linewidth]{figures/fig7.eps}
     \caption*{\textnormal{Adjustments of simulation-based decision thresholds for multiple tests.}}
\end{figure}
```
<br>
`\subsubsection{Overlap of False Positive Results}` 
In order to check whether inference methods come to diverging conclusions for the same models, we computed the overlap of false positive results. With simulation-based decision thresholds, `r proportion_to_percentage(ofp_savage_dickey_bf_directional_bf)``\%` of false positive Savage-Dickey BFs overlapped with false positive dBFs, `r proportion_to_percentage(ofp_savage_dickey_bf_hdi)``\%` overlapped with false positive HDI decisions and `r proportion_to_percentage(ofp_savage_dickey_bf_p_effect)``\%` overlapped with false positive P(effect > 0) proportions. `r proportion_to_percentage(ofp_directional_bf_hdi)``\%` of false positive dBFs overlapped with false positive HDI decisions and `r proportion_to_percentage(ofp_directional_bf_p_effect)``\%` overlapped with false positive P(effect > 0) proportions. `r proportion_to_percentage(ofp_hdi_p_effect)``\%` of false positive HDI decisions overlapped with false positive P(effect > 0) proportions. This shows that even with comparable false positive risk, the Savage-Dickey BF and inference methods that directly depend on the posterior distribution, namely the dBF, P(effect > 0), and the HDI against zero decision rule, can come to diverging conclusions.
`\end{document}`
<br>
